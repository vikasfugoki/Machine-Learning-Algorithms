{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python sklearn also provide the implementation for the Logistic Regression. Below is code of how we can use the python implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self, k, n_iter=100, lr=0.01):\n",
    "        self.w = []\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # for bias term\n",
    "        w_0 = np.ones(len(X)).reshape(len(X), 1)\n",
    "        X = np.concatenate((w_0, X), axis=1)\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # initialize the w\n",
    "        self.w = np.random.randn(D * self.k).reshape(D, self.k)\n",
    "        \n",
    "        for j in range(self.n_iter):\n",
    "            s_vec = np.dot(X, self.w) # score\n",
    "            f = lambda x : np.exp(x) \n",
    "            s_vec_exp = f(s_vec)\n",
    "            p = s_vec_exp\n",
    "            #p = normalise(s_vec_exp)\n",
    "            \n",
    "            row_sums = p.sum(axis=1) # sum along colums of each row\n",
    "            nor_p = p / row_sums[:, np.newaxis] # normalzation\n",
    "            p = nor_p\n",
    "            \n",
    "            # calculate the gradient\n",
    "            grad = 1/ N * np.dot((p - y).T, X)\n",
    "            self.w = self.w - self.lr * grad.T # updating the parameters\n",
    "            \n",
    "    def predict(self, X):\n",
    "        w_0 = np.ones(len(X)).reshape(len(X), 1)\n",
    "        X = np.concatenate((w_0, X), axis=1)\n",
    "        N, D = X.shape\n",
    "        f = lambda x : np.exp(x)\n",
    "        s_vec_pred = np.dot(X, self.w)\n",
    "        p = f(s_vec_pred)\n",
    "        \n",
    "        #row_sums = p.sum(axis=1) # sum along colums of each row\n",
    "        #nor_p = p / row_sums[:, np.newaxis] # normalzation\n",
    "        \n",
    "        predictions = []\n",
    "        for p_row in p:\n",
    "            ind = np.argmax(p_row)\n",
    "            predictions.append(ind)\n",
    "            \n",
    "        predictions = np.array(predictions)\n",
    "        #print(nor_p)\n",
    "        return predictions\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_enc = encoder.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_reg = Softmax(3, 1000)\n",
    "soft_reg.fit(X,y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2,\n",
       "       2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = soft_reg.predict(X)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, multi_class='multinomial', random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python implementation of LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = softmax_reg.predict(X)\n",
    "predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
